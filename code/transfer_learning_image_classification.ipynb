{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google Colab-related code"
      ],
      "metadata": {
        "id": "YqS1kUhA3s74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/ai_image_classification_cifar/code')\n",
        "%cd /content/drive/My\\ Drive/ai_image_classification_cifar/code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXnwoQfxYVHI",
        "outputId": "68738c49-ee7f-47f3-d8b4-5a289953f446"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/ai_image_classification_cifar/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up environment"
      ],
      "metadata": {
        "id": "GMElIw-k4H0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install necessary dependencies\n",
        "#!bash install-dependencies.sh\n",
        "\n",
        "#Install packages\n",
        "from required_packages import *"
      ],
      "metadata": {
        "id": "4tbnzbxKc4Bg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source images"
      ],
      "metadata": {
        "id": "wZu0tCVozz44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this code when sourcing images from Kaggle account. However, do not run this when images are already sourced.\n",
        "class Images:\n",
        "    '''\n",
        "    A class to source images.\n",
        "    '''\n",
        "    def __init__(self, num_images):\n",
        "        self.num_images = num_images\n",
        "        self.orig_dir = '/kaggle/input/cifake-real-and-ai-generated-synthetic-images'\n",
        "        self.dest_dir = '/kaggle/working/cifake'\n",
        "\n",
        "    def copy_images(self):\n",
        "        categories = ['FAKE', 'REAL']\n",
        "        dataset_type = ['train', 'test']\n",
        "\n",
        "        #Copy train & test images\n",
        "        for i in dataset_type:\n",
        "            for j in categories:\n",
        "                orig_dir = os.path.join(self.orig_dir, i, j)\n",
        "                dest_dir = os.path.join(self.dest_dir, i, j)\n",
        "                functions.source_images(orig_dir = orig_dir, dest_dir = dest_dir, num_images = self.num_images, seed = 23)\n",
        "        #Copy validation images\n",
        "        for j in categories:\n",
        "            train_dir= os.path.join(self.dest_dir, 'train', j)\n",
        "            validation_dir = '/kaggle/working/cifake/validation'\n",
        "\n",
        "            all_files = os.listdir(train_dir)\n",
        "            random.seed(23)\n",
        "            selected_files = random.sample(all_files, 100)\n",
        "\n",
        "            for file in selected_files:\n",
        "                train_file_path = os.path.join(train_dir, file)\n",
        "                validation_file_path = os.path.join(validation_dir, j, file)\n",
        "                os.makedirs(validation_file_path, exist_ok=True)\n",
        "                shutil.copy(train_file_path, validation_file_path)\n",
        "\n",
        "                os.remove(train_file_path)"
      ],
      "metadata": {
        "id": "seiCDGM58UGv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess images"
      ],
      "metadata": {
        "id": "e4OYEhBQ488-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocess:\n",
        "    def __init__(self, **mdict):\n",
        "        self.mdict = mdict\n",
        "\n",
        "    def create_generators(self):\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rescale = self.mdict['generators']['rescale'],\n",
        "            rotation_range = self.mdict['generators']['rotation_range'],\n",
        "            width_shift_range = self.mdict['generators']['width_shift_range'],\n",
        "            height_shift_range = self.mdict['generators']['height_shift_range'],\n",
        "            shear_range = self.mdict['generators']['shear_range'],\n",
        "            zoom_range = self.mdict['generators']['zoom_range'],\n",
        "            fill_mode = self.mdict['generators']['fill_mode'])\n",
        "\n",
        "        train_generator = train_datagen.flow_from_directory(\n",
        "            self.mdict['info']['train_dir'],\n",
        "            target_size = (224, 224),\n",
        "            batch_size = 32,\n",
        "            classes = self.mdict['info']['classes'],\n",
        "            class_mode = 'binary')\n",
        "\n",
        "        validation_generator = ImageDataGenerator().flow_from_directory(\n",
        "            self.mdict['info']['validation_dir'],\n",
        "            target_size = (224, 224),\n",
        "            batch_size = 32,\n",
        "            classes = self.mdict['info']['classes'],\n",
        "            class_mode = 'binary')\n",
        "\n",
        "        test_generator = ImageDataGenerator().flow_from_directory(\n",
        "            self.mdict['info']['test_dir'],\n",
        "            target_size = (224, 224),\n",
        "            batch_size = 32,\n",
        "            classes = self.mdict['info']['classes'],\n",
        "            shuffle = False)\n",
        "\n",
        "        return train_generator, validation_generator, test_generator"
      ],
      "metadata": {
        "id": "xqutCs4448mo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load YAML file and create training, validation, and test datasets"
      ],
      "metadata": {
        "id": "vBehcWNydZU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python create_yaml_files.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHwK30Lx8dn6",
        "outputId": "ef8e32be-b1f0-4268-919b-b0dd92c3dd40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionaries updated and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning from pre-trained MobileNet V2 Model (Google)\n"
      ],
      "metadata": {
        "id": "Z1oBjwrJE5-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "class TransferLearning:\n",
        "    def __init__(self, train_generator, validation_generator, **mdict):  # Added validation_generator\n",
        "        self.mdict = mdict\n",
        "        self.train_generator = train_generator\n",
        "        self.validation_generator = validation_generator  # Added validation_generator\n",
        "\n",
        "    def create_base_model(self):\n",
        "        IMG_SIZE = tuple(self.mdict['preprocess']['resize'])\n",
        "        IMG_SHAPE = IMG_SIZE + (3,)\n",
        "        # Ensure the method name is correctly used and called\n",
        "        base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                                     include_top=False,\n",
        "                                                     weights='imagenet')\n",
        "        return base_model\n",
        "\n",
        "    def add_classification_layer(self):\n",
        "        # Corrected typo in method call (create_base_mode -> create_base_model)\n",
        "        base_model = self.create_base_model()\n",
        "        base_model.trainable = False  # Freeze the convolutional base\n",
        "\n",
        "        # Identify feature batch\n",
        "        image_batch, label_batch = next(iter(self.train_generator))\n",
        "        feature_batch = base_model(image_batch)\n",
        "\n",
        "        # Add GlobalAveragePooling2D and Dense layer for classification\n",
        "        global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        feature_batch_average = global_average_layer(feature_batch)\n",
        "\n",
        "        # Apply Dense layer to convert features into a single prediction per image\n",
        "        prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        prediction_batch = prediction_layer(feature_batch_average)\n",
        "\n",
        "        # Create new model by chaining the layers\n",
        "        preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "\n",
        "        IMG_SIZE = tuple(self.mdict['preprocess']['resize'])\n",
        "\n",
        "        inputs = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "        x = preprocess_input(inputs)\n",
        "        x = base_model(inputs, training=False)  # Pass inputs through the base model\n",
        "        x = global_average_layer(x)  # Apply global average pooling\n",
        "        x = tf.keras.layers.Dropout(0.2)(x)\n",
        "        outputs = prediction_layer(x)  # Apply the dense prediction layer\n",
        "\n",
        "        # Define the complete model\n",
        "        final_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "        return final_model\n",
        "\n",
        "    def compile_model(self):\n",
        "        # Corrected to call add_classification_layer to get the final model\n",
        "        tf_model = self.add_classification_layer()\n",
        "\n",
        "        # Fetch the optimizer class using getattr\n",
        "        learning_rate = self.mdict['transfer_learning']['learning_rate']\n",
        "\n",
        "        optimizer_class = getattr(tf.keras.optimizers, self.mdict['transfer_learning']['optimizer'])\n",
        "        loss_class = getattr(tf.keras.losses, self.mdict['transfer_learning']['loss'])\n",
        "        metrics_class = getattr(tf.keras.metrics, self.mdict['transfer_learning']['metrics']['name'])\n",
        "\n",
        "        tf_model.compile(loss= loss_class(**{}),\n",
        "                         optimizer=optimizer_class(learning_rate=df_dict['transfer_learning']['learning_rate']),\n",
        "                         metrics= [metrics_class(threshold=self.mdict['transfer_learning']['metrics']['params']['threshold'], name=df_dict['transfer_learning']['metrics']['params']['name'])])\n",
        "        return tf_model\n",
        "\n",
        "    def fit_model(self):\n",
        "        # Ensure model is compiled and returned properly\n",
        "        tf_model = self.compile_model()\n",
        "\n",
        "        # Fixed indentation and variable reference issues\n",
        "        log_dir = \"../output/logs/\" + self.mdict['info']['model_name']\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "        # Ensure correct model and training call\n",
        "        history = tf_model.fit(\n",
        "            self.train_generator,  # Train generator\n",
        "            epochs=self.mdict['transfer_learning']['initial_epochs'],\n",
        "            validation_data=self.validation_generator,\n",
        "            callbacks=[tensorboard_callback]\n",
        "        )\n",
        "\n",
        "        return history, tf_model  # Return both history and model\n",
        "\n",
        "    def save_model(self):\n",
        "        # Ensure model and history are saved correctly\n",
        "        history, model = self.fit_model()\n",
        "        model.save(self.mdict['info']['model_filepath'])\n",
        "\n",
        "        with open(self.mdict['info']['history_filepath'], 'wb') as file:\n",
        "            pickle.dump(history.history, file)"
      ],
      "metadata": {
        "id": "cVkmA4qJuOyH"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()  # Start timing\n",
        "\n",
        "yaml_file = '../input/base_tf_dict.yaml'\n",
        "\n",
        "with open(yaml_file, 'r') as file:\n",
        "    df_dict = yaml.safe_load(file)\n",
        "\n",
        "generator = Preprocess(**df_dict)\n",
        "train_generator, validation_generator, test_generator = generator.create_generators()\n",
        "tf_model = TransferLearning(train_generator, validation_generator, **df_dict)\n",
        "tf_model.save_model()\n",
        "end_time = time.time()  # End timing\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(elapsed_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrF5MrcJKpQY",
        "outputId": "99a956a5-d0d4-49bc-a34d-26f8677fc21a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 800 images belonging to 2 classes.\n",
            "Found 735 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "25/25 [==============================] - 21s 764ms/step - loss: 0.7092 - accuracy: 0.6100 - val_loss: 0.7473 - val_accuracy: 0.4680\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 18s 726ms/step - loss: 0.5917 - accuracy: 0.6975 - val_loss: 0.7911 - val_accuracy: 0.4816\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 18s 717ms/step - loss: 0.5536 - accuracy: 0.7337 - val_loss: 0.8268 - val_accuracy: 0.4857\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 18s 725ms/step - loss: 0.5485 - accuracy: 0.7125 - val_loss: 0.8147 - val_accuracy: 0.4884\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 18s 719ms/step - loss: 0.5217 - accuracy: 0.7425 - val_loss: 0.8895 - val_accuracy: 0.4803\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 18s 713ms/step - loss: 0.5037 - accuracy: 0.7412 - val_loss: 0.8255 - val_accuracy: 0.4803\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 18s 729ms/step - loss: 0.5047 - accuracy: 0.7625 - val_loss: 0.7868 - val_accuracy: 0.4735\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 18s 725ms/step - loss: 0.4977 - accuracy: 0.7613 - val_loss: 0.8577 - val_accuracy: 0.4884\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 18s 715ms/step - loss: 0.4763 - accuracy: 0.7613 - val_loss: 0.9039 - val_accuracy: 0.4830\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 18s 712ms/step - loss: 0.4807 - accuracy: 0.7625 - val_loss: 0.8543 - val_accuracy: 0.4816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "187.99368834495544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning of model"
      ],
      "metadata": {
        "id": "SndnmtpdUKgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FineTuning:\n",
        "    def __init__(self, tf_model, train_generator, validation_generator, **mdict):\n",
        "      self.mdict = mdict\n",
        "      self.tf_model = tf_model\n",
        "      self.train_generator = train_generator\n",
        "      self.validation_generator = validation_generator\n",
        "\n",
        "    def identify_ft_layers(self):\n",
        "\n",
        "      IMG_SIZE = tuple(self.mdict['preprocess']['resize'])\n",
        "      IMG_SHAPE = IMG_SIZE + (3,)\n",
        "\n",
        "      self.tf_model.trainable = True\n",
        "      fine_tune_at = self.mdict['fine_tuning']['fine_tune_at']\n",
        "\n",
        "      # Freeze all the layers before the `fine_tune_at` layer\n",
        "      for layer in tf_model.layers[:fine_tune_at]:\n",
        "        layer.trainable = False\n",
        "\n",
        "      optimizer_class = getattr(tf.keras.optimizers, self.mdict['fine_tuning']['optimizer'])\n",
        "      loss_class = getattr(tf.keras.losses, self.mdict['fine_tuning']['loss'])\n",
        "      metrics_class = getattr(tf.keras.metrics, self.mdict['fine_tuning']['metrics']['name'])\n",
        "\n",
        "      tf_model.compile(loss= loss_class(**{}),\n",
        "                    optimizer=optimizer_class(learning_rate=df_dict['fine_tuning']['learning_rate']),\n",
        "                    metrics= [metrics_class(threshold=self.mdict['fine_tuning']['metrics']['params']['threshold'], name=df_dict['fine_tuning']['metrics']['params']['name'])])\n",
        "\n",
        "      return tf_model\n",
        "\n",
        "    def fit_model(self):\n",
        "      ft_model = self.identify_ft_layers()\n",
        "      fine_tune_epochs = self.mdict['fine_tuning']['fine_tune_epochs']\n",
        "      total_epochs = self.mdict['fine_tuning']['fine_tune_epochs'] + self.mdict['transfer_learning']['initial_epochs']\n",
        "\n",
        "      log_dir = \"../output/logs/\" + self.mdict['info']['model_name']\n",
        "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "      # Ensure correct model and training call\n",
        "      history = ft_model.fit(\n",
        "          self.train_generator,  # Train generator\n",
        "          epochs=total_epochs,\n",
        "          initial_epoch = self.mdict['transfer_learning']['initial_epochs'],\n",
        "          validation_data=self.validation_generator,\n",
        "          callbacks=[tensorboard_callback]\n",
        "      )\n",
        "\n",
        "      return history, ft_model\n",
        "\n",
        "    def save_model(self):\n",
        "        history, ft_model = self.fit_model()\n",
        "        ft_model.save(self.mdict['info']['finetune_filepath'])\n",
        "\n",
        "        with open(self.mdict['info']['finetune_history_filepath'], 'wb') as file:\n",
        "            pickle.dump(history.history, file)"
      ],
      "metadata": {
        "id": "_gsSXDDWVCPY"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()  # Start timing\n",
        "ft_model = FineTuning(tf_model, train_generator, validation_generator, **df_dict)\n",
        "ft_model.save_model()\n",
        "end_time = time.time()  # End timing\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(elapsed_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyyLK6xpMqpj",
        "outputId": "8141fc04-05a8-4373-cf7e-2ece256ea3ea"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20\n",
            "25/25 [==============================] - 20s 756ms/step - loss: 0.4513 - accuracy: 0.7912 - val_loss: 0.8543 - val_accuracy: 0.4816\n",
            "Epoch 12/20\n",
            "25/25 [==============================] - 18s 719ms/step - loss: 0.4840 - accuracy: 0.7613 - val_loss: 0.8543 - val_accuracy: 0.4816\n",
            "Epoch 13/20\n",
            "25/25 [==============================] - 18s 717ms/step - loss: 0.4645 - accuracy: 0.7800 - val_loss: 0.8543 - val_accuracy: 0.4816\n",
            "Epoch 14/20\n",
            "25/25 [==============================] - 18s 721ms/step - loss: 0.4952 - accuracy: 0.7487 - val_loss: 0.8543 - val_accuracy: 0.4816\n",
            "Epoch 15/20\n",
            "25/25 [==============================] - 18s 728ms/step - loss: 0.4724 - accuracy: 0.7850 - val_loss: 0.8543 - val_accuracy: 0.4816\n",
            "Epoch 16/20\n",
            "25/25 [==============================] - 18s 724ms/step - loss: 0.4693 - accuracy: 0.7825 - val_loss: 0.8543 - val_accuracy: 0.4816\n",
            "Epoch 17/20\n",
            "25/25 [==============================] - 18s 735ms/step - loss: 0.4804 - accuracy: 0.7837 - val_loss: 0.8543 - val_accuracy: 0.4816\n",
            "Epoch 18/20\n",
            "25/25 [==============================] - 18s 720ms/step - loss: 0.4802 - accuracy: 0.7688 - val_loss: 0.8543 - val_accuracy: 0.4816\n",
            "Epoch 19/20\n",
            "25/25 [==============================] - 18s 718ms/step - loss: 0.4990 - accuracy: 0.7575 - val_loss: 0.8543 - val_accuracy: 0.4816\n",
            "Epoch 20/20\n",
            "25/25 [==============================] - 18s 722ms/step - loss: 0.4719 - accuracy: 0.7675 - val_loss: 0.8543 - val_accuracy: 0.4816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "182.65896105766296\n"
          ]
        }
      ]
    }
  ]
}