{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 5256696,
          "sourceType": "datasetVersion",
          "datasetId": 3041726
        },
        {
          "sourceId": 8627915,
          "sourceType": "datasetVersion",
          "datasetId": 5165641
        },
        {
          "sourceId": 8650945,
          "sourceType": "datasetVersion",
          "datasetId": 5181871
        },
        {
          "sourceId": 8687958,
          "sourceType": "datasetVersion",
          "datasetId": 5121028
        },
        {
          "sourceId": 183208323,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 62076,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 51855
        },
        {
          "sourceId": 62077,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 51856
        },
        {
          "sourceId": 62078,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 51857
        },
        {
          "sourceId": 62079,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 51858
        },
        {
          "sourceId": 62080,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 51859
        }
      ],
      "dockerImageVersionId": 30715,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/ai_image_classification_cifar/code')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2dC06WJFITo",
        "outputId": "01e19963-6915-47aa-d4c6-f0ad9c045b4a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/My\\ Drive/ai_image_classification_cifar/code"
      ],
      "metadata": {
        "id": "LajBs1xw91iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages"
      ],
      "metadata": {
        "id": "c5BrkTxyFGRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from fastcore.all import *\n",
        "from fastai.vision.all import *\n",
        "from time import sleep\n",
        "from pathlib import Path\n",
        "\n",
        "from fastdownload import download_url\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pickle\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "import seaborn as sns\n",
        "\n",
        "from functions import *"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-06-14T05:11:35.734024Z",
          "iopub.execute_input": "2024-06-14T05:11:35.734671Z",
          "iopub.status.idle": "2024-06-14T05:11:51.273914Z",
          "shell.execute_reply.started": "2024-06-14T05:11:35.734618Z",
          "shell.execute_reply": "2024-06-14T05:11:51.272182Z"
        },
        "trusted": true,
        "id": "GVzs_ZxBFGRS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create & import YAML files"
      ],
      "metadata": {
        "id": "Suw7ZQ_lKIGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/ai_image_classification_cifar/code\n",
        "!python create_yaml_files.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbcAsUxPKN7k",
        "outputId": "be8fe618-af7e-4403-f026-0935ca53cb34"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/ai_image_classification_cifar/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('../input/base_dict.yaml', 'r') as file:\n",
        "    base_dict = yaml.safe_load(file)"
      ],
      "metadata": {
        "id": "9fGvqLXlTtWy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source images"
      ],
      "metadata": {
        "id": "a5UnW7OVFGRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Originally ran this code when sourcing images from Kaggle account. However, do not run this when images are already sourced.\n",
        "class Images:\n",
        "    '''\n",
        "    A class to source images.\n",
        "    '''\n",
        "    def __init__(self, num_images):\n",
        "        self.num_images = num_images\n",
        "        self.orig_dir = '/kaggle/input/cifake-real-and-ai-generated-synthetic-images'\n",
        "        self.dest_dir = '/kaggle/working/cifake'\n",
        "\n",
        "    def copy_images(self):\n",
        "        categories = ['FAKE', 'REAL']\n",
        "        dataset_type = ['train', 'test']\n",
        "\n",
        "        #Copy train & test images\n",
        "        for i in dataset_type:\n",
        "            for j in categories:\n",
        "                orig_dir = os.path.join(self.orig_dir, i, j)\n",
        "                dest_dir = os.path.join(self.dest_dir, i, j)\n",
        "                source_images(orig_dir = orig_dir, dest_dir = dest_dir, num_images = self.num_images, seed = 23)\n",
        "        #Copy validation images\n",
        "        for j in categories:\n",
        "            train_dir= os.path.join(self.dest_dir, 'train', j)\n",
        "            validation_dir = '/kaggle/working/cifake/validation'\n",
        "\n",
        "            all_files = os.listdir(train_dir)\n",
        "            random.seed(23)\n",
        "            selected_files = random.sample(all_files, 100)\n",
        "\n",
        "            for file in selected_files:\n",
        "                train_file_path = os.path.join(train_dir, file)\n",
        "                validation_file_path = os.path.join(validation_dir, j, file)\n",
        "                os.makedirs(validation_file_path, exist_ok=True)\n",
        "                shutil.copy(train_file_path, validation_file_path)\n",
        "\n",
        "                os.remove(train_file_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T05:11:51.277633Z",
          "iopub.execute_input": "2024-06-14T05:11:51.278636Z",
          "iopub.status.idle": "2024-06-14T05:11:51.295380Z",
          "shell.execute_reply.started": "2024-06-14T05:11:51.278581Z",
          "shell.execute_reply": "2024-06-14T05:11:51.293671Z"
        },
        "trusted": true,
        "id": "d0s5WiFfFGRU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess images"
      ],
      "metadata": {
        "id": "exyQvbRBFGRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocess:\n",
        "    def __init__(self, **mdict):\n",
        "        self.mdict = mdict\n",
        "\n",
        "    def create_generators(self):\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rescale = self.mdict['generators']['rescale'],\n",
        "            rotation_range = self.mdict['generators']['rotation_range'],\n",
        "            width_shift_range = self.mdict['generators']['width_shift_range'],\n",
        "            height_shift_range = self.mdict['generators']['height_shift_range'],\n",
        "            shear_range = self.mdict['generators']['shear_range'],\n",
        "            zoom_range = self.mdict['generators']['zoom_range'],\n",
        "            fill_mode = self.mdict['generators']['fill_mode'])\n",
        "\n",
        "        train_generator = train_datagen.flow_from_directory(\n",
        "            self.mdict['info']['train_dir'],\n",
        "            target_size = (224, 224),\n",
        "            batch_size = 32,\n",
        "            classes = self.mdict['info']['classes'])\n",
        "\n",
        "        validation_generator = ImageDataGenerator().flow_from_directory(\n",
        "            self.mdict['info']['validation_dir'],\n",
        "            target_size = (224, 224),\n",
        "            batch_size = 32,\n",
        "            classes = self.mdict['info']['classes'])\n",
        "\n",
        "        test_generator = ImageDataGenerator().flow_from_directory(\n",
        "            self.mdict['info']['test_dir'],\n",
        "            target_size = (224, 224),\n",
        "            batch_size = 32,\n",
        "            classes = self.mdict['info']['classes'],\n",
        "            shuffle = False)\n",
        "\n",
        "        return train_generator, validation_generator, test_generator"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T05:11:51.297596Z",
          "iopub.execute_input": "2024-06-14T05:11:51.298141Z",
          "iopub.status.idle": "2024-06-14T05:11:51.320810Z",
          "shell.execute_reply.started": "2024-06-14T05:11:51.298101Z",
          "shell.execute_reply": "2024-06-14T05:11:51.319527Z"
        },
        "trusted": true,
        "id": "Y1FGyokIFGRV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Preprocess(**base_dict)\n",
        "train_generator, validation_generator, test_generator = generator.create_generators()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKip5MPaVIQa",
        "outputId": "895930db-4641-41a2-a07e-853f60eeca4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 800 images belonging to 2 classes.\n",
            "Found 735 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and save CNN models"
      ],
      "metadata": {
        "id": "HrsXtLRWNo18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_model:\n",
        "    def __init__(self, train_generator, validation_generator, **mdict):\n",
        "        self.train_generator = train_generator\n",
        "        self.validation_generator = validation_generator\n",
        "        self.mdict = mdict\n",
        "\n",
        "    def define_architecture(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        #Convolutional layers\n",
        "        for i, conv in enumerate(self.mdict['conv_layers']):\n",
        "            if i==0:\n",
        "                model.add(Conv2D(\n",
        "                    filters = conv['filters'],\n",
        "                    kernel_size = conv['kernel_size'],\n",
        "                    activation =conv['activation'],\n",
        "                    input_shape= conv['input_shape']\n",
        "                ))\n",
        "            else:\n",
        "                model.add(Conv2D(\n",
        "                    filters = conv['filters'],\n",
        "                    kernel_size = conv['kernel_size'],\n",
        "                    activation =conv['activation']\n",
        "                ))\n",
        "\n",
        "            model.add(MaxPooling2D(self.mdict['maxpool_layers']['pool_size']))\n",
        "\n",
        "        #Flatten\n",
        "        model.add(Flatten())\n",
        "\n",
        "        #Dropout\n",
        "        model.add(Dropout(self.mdict['model']['dropout']))\n",
        "\n",
        "        #Dense layers\n",
        "        model.add(Dense(\n",
        "            units = self.mdict['dense_layers']['units'],\n",
        "            activation = self.mdict['dense_layers']['activation']))\n",
        "\n",
        "        #Output\n",
        "        model.add(Dense(\n",
        "            units = self.mdict['output_layer']['units'],\n",
        "            activation = self.mdict['output_layer']['activation']))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def compile_model(self):\n",
        "        model = self.define_architecture()\n",
        "        model.compile(\n",
        "            loss= self.mdict['model']['loss'],\n",
        "            optimizer= self.mdict['model']['optimizer'],\n",
        "            metrics=self.mdict['model']['metrics'])\n",
        "        return model\n",
        "\n",
        "    def fit_model (self):\n",
        "        model = self.compile_model()\n",
        "\n",
        "        log_dir = \"../output/logs/\" + self.mdict['info']['model_name']\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "        history = model.fit(train_generator,\n",
        "                            steps_per_epoch = self.mdict['model']['steps_per_epoch'],\n",
        "                            epochs = self.mdict['model']['epochs'],\n",
        "                            validation_data = self.validation_generator,\n",
        "                            validation_steps = self.mdict['model']['validation_steps'],\n",
        "                            callbacks=[tensorboard_callback])\n",
        "\n",
        "        return history, model\n",
        "\n",
        "    def save_model(self):\n",
        "        history, model = self.fit_model()\n",
        "        model.save(self.mdict['info']['model_filepath'])\n",
        "\n",
        "        with open(self.mdict['info']['history_filepath'], 'wb') as file:\n",
        "            pickle.dump(history.history, file)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T05:11:51.323931Z",
          "iopub.execute_input": "2024-06-14T05:11:51.325492Z",
          "iopub.status.idle": "2024-06-14T05:11:51.344967Z",
          "shell.execute_reply.started": "2024-06-14T05:11:51.325436Z",
          "shell.execute_reply": "2024-06-14T05:11:51.343437Z"
        },
        "trusted": true,
        "id": "KR_NML5qFGRV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base = CNN_model(train_generator, validation_generator, **base_dict)\n",
        "base.save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJc3A6RoOM1-",
        "outputId": "3eb44900-3ea4-40b7-e814-459a4a3edfb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            " 25/100 [======>.......................] - ETA: 21:57 - loss: 2.8146 - accuracy: 0.5100"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate models"
      ],
      "metadata": {
        "id": "pU_072siFGRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluate_model:\n",
        "    def __init__(self, test_generator):\n",
        "        self.test_generator = test_generator\n",
        "        self.test_table = pd.DataFrame()\n",
        "        self.histories = []\n",
        "        self.history_names = []\n",
        "\n",
        "    def add_history(self, history, name):\n",
        "        self.histories.append(history)\n",
        "        self.history_names.append(name)\n",
        "\n",
        "    def visualize_training(self, metrics):\n",
        "        calc_histories(metrics, self.histories, self.history_names)\n",
        "\n",
        "    def calc_metrics(self, model, name):\n",
        "        row, accuracy, precision, recall, f1, y_true, y_pred_classes, y_pred = calc_eval_metrics(model, name, self.test_generator)\n",
        "        self.test_table = pd.concat([self.test_table, row], ignore_index = True)\n",
        "        return accuracy, precision, recall, f1, y_true, y_pred_classes, y_pred\n",
        "\n",
        "    def plot_confusion_matrix(self, model, name):\n",
        "        accuracy, precision, recall, f1, y_true, y_pred_classes, y_pred = self.calc_metrics(model, name)\n",
        "        calc_confusion_matrix(y_true, y_pred_classes, name)\n",
        "\n",
        "    def plot_roc_curve(self, model, name):\n",
        "        accuracy, precision, recall, f1, y_true, y_pred_classes, y_pred = self.calc_metrics(model, name)\n",
        "        n_classes = self.test_generator.num_classes\n",
        "        y_test_bin = label_binarize(y_true, classes=[0, 1])\n",
        "\n",
        "        calc_roc_curve(n_classes, y_true, y_pred, name)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-14T05:12:04.258549Z",
          "iopub.execute_input": "2024-06-14T05:12:04.259073Z",
          "iopub.status.idle": "2024-06-14T05:12:04.277714Z",
          "shell.execute_reply.started": "2024-06-14T05:12:04.259005Z",
          "shell.execute_reply": "2024-06-14T05:12:04.276225Z"
        },
        "trusted": true,
        "id": "zpeqd1tAFGRW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run evaluation"
      ],
      "metadata": {
        "id": "qkOVN40lFGRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_CNN_model(**mdict):\n",
        "    generators = Preprocess(**mdict)\n",
        "    train_generator, validation_generator, test_generator = generators.create_generators()\n",
        "    model = CNN_model(train_generator, validation_generator, **mdict)\n",
        "    model.save_model()\n",
        "\n",
        "train_CNN_model(**base_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnyX_NbbUL1y",
        "outputId": "c2c70317-e245-40c9-a030-efac8538a760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 800 images belonging to 2 classes.\n",
            "Found 735 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            " 25/100 [======>.......................] - ETA: 5:54 - loss: 1.4169 - accuracy: 0.5700"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_eval(test_generator, **mdict):\n",
        "    model = load_model(mdict['info']['model_filepath'])\n",
        "    with open(mdict['info']['history_filepath'], 'rb') as file:\n",
        "        history = pickle.load(file)\n",
        "\n",
        "    model_eval = Evaluate_model(test_generator)\n",
        "    model_eval.calc_metrics(model, mdict['info']['model_name'])\n",
        "    model_eval.add_history(history, mdict['info']['model_name'])\n",
        "    model_eval.plot_confusion_matrix(model, mdict['info']['model_name'])\n",
        "    model_eval.plot_roc_curve(model, mdict['info']['model_name'])\n",
        "\n",
        "run_eval(test_generator, base_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "-plShTwGN6K-",
        "outputId": "46b43b2f-a59a-4890-b68f-fea02a399551"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_generator' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a0acd3c3dfbb>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_roc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrun_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_generator' is not defined"
          ]
        }
      ]
    }
  ]
}